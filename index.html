<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shengjie Wang</title>
  
  <meta name="author" content="Shengjie Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
  <style>
    .yellow-row {
        background-color: rgb(224, 255, 207);
    }
    </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shengjie Wang | 王圣杰</name>
              </p>
              <p>I am a PhD student in Computer Science at <a href="https://iiis.tsinghua.edu.cn/en/">Institute for Interdisciplinary Information Sciences (IIIS)</a>, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University </a>, working with Prof. <a href="http://people.iiis.tsinghua.edu.cn/~gaoyang/yang-gao.weebly.com/index.html">Yang Gao</a>. Previously, I was a Master's student in <a href=https://www.au.tsinghua.edu.cn/#>Department of Automation</a> at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University </a>, advised by Prof. <a href="https://www.au.tsinghua.edu.cn/info/1110/1573.htm">Tao Zhang</a>(IEEE Fellow & IET Fellow, Head of Department). I completed my Bachelor's in Robotics at BIT, supervised by Prof. <a href="https://sites.google.com/view/qingshi/home">Qing Shi</a> (IEEE Senior Member) and Prof. <a href="http://toshiofukuda.org">Toshio Fukuda</a> (2020 IEEE President).
              </p>
              <p style="text-align:center">
                <a href="mailto:wangsj23@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=ghuIBIYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://x.com/ShengjieWa34067">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/Shengjiewang-Jason">Github</a>&nbsp/&nbsp
<!--                 <a href="https://www.linkedin.com/in/jason-shengjie-wang-03052a268/">Linkedin</a>&nbsp/&nbsp -->
                <a href="images/Wechat.jpg">WeChat</a>
              </p>
            </td>

            <td style="padding:2.5%;width:40%;max-width:40%">
              <br>
              <a href="images/shengjiewang.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/icml_photo.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>
                <span class="php">Research Interests</span></heading>
              <p>
                My research interests lie in the intersection of <strong>Robotics</strong> and <strong>Reinforcement Learning</strong>. I care about efficient, stable and safe robot performance in the unstructured open world.
              </p>
              <p>
                Furthermore, we are passionate about releasing some interesting robotic environments, such as <a href=https://github.com/PKU-MARL/DexterousHands>Bi-DexHands</a> and <a href=https://github.com/Tsinghua-Space-Robot-Learning-Group/SpaceRobotEnv>SpaceRobotEnv</a>. Hope everyone enjoys our work!
              </p>
              <br>
          
                <!-- <img src='images/coverv4.jpg' width="700"> -->
        </tr>
        </td>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>
              <span class="php">Selected Publications</span></heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/SKIL.gif' width="100%">
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://skil-robotics.github.io/SKIL-robotics/">
              <papertitle>SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation</papertitle>
            </a>
            <br>
            <strong>Shengjie Wang</strong>,
            Jiacheng You,
            Yihang Hu,
            Jiongye Li,
            Yang Gao
              <br>
              <span style="font-size: 16px;color: red;"><em>CoRL</em></span>, 2024</span>
              <br>
              <a href="https://skil-robotics.github.io/SKIL-robotics/">Project Page</a>
              /
              <a href="https://arxiv.org/abs/2501.14400">ArXiv</a>
              /
              <a href="https://skil-robotics.github.io/SKIL-robotics/">Code</a>
              /
              <!-- <a href="https://youtu.be/2mmqQYO4KlY">Video</a> -->
              <p></p>
              <p>
                We propose Semantic Keypoint Imitation Learning (SKIL), a framework which automatically obtains semantic keypoints with the help of vision foundation models, and forms the descriptor of semantic keypoints that enables efficient imitation learning of complex robotic tasks with significantly lower sample complexity.
              </p>
          </td>
        </tr>

	      
        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/robometaverse.gif' width="100%">
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://mp.weixin.qq.com/s/Lyy4Fn07ERz2HNSFpyxU8Q">
              <papertitle>RoboMetaverse: A Large-scale Robotic Metaverse For Reinforcement Learning</papertitle>
            </a>
            <br>
            Fengbo Lan*, 
            Chu Zhang*, 
            Ziyan Zhang*,
            <strong>Shengjie Wang*</strong>,
	    Haotian Xu,
	    Yunzhe Zhang,
            Tao Zhang
              <br>
              <span style="font-size: 16px;color: red;"><em>A Cool Work</em></span>, 2024</span>
              <br>
              <a href="https://mp.weixin.qq.com/s/Lyy4Fn07ERz2HNSFpyxU8Q">Project website</a>
              / 
              <a href="https://mp.weixin.qq.com/s/Lyy4Fn07ERz2HNSFpyxU8Q">Video</a>
              /
<!--               <a href="https://arxiv.org/abs/2403.08248">ArXiv</a>
              /
              <a href="https://copa-2024.github.io/">Code</a>
              / -->
              <!-- <a href="https://youtu.be/2mmqQYO4KlY">Video</a> -->
              <p></p>
              <p>
                RoboMetaverse is founded by a team of robotics enthusiasts, dedicated to creating a highly realistic embodied intelligence simulation platform and a cost-effective embodied intelligence hardware platform, aiming to contribute to the realization of the AGI (Artificial General Intelligence) era in embodied intelligence.
              </p>
          </td>
        </tr>

        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/dexcatch.gif' width="100%">
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://dexcatch.github.io/">
              <papertitle>DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands</papertitle>
            </a>
            <br>
            Fengbo Lan*,
            <strong>Shengjie Wang*</strong>,
            Yunzhe Zhang,
            Haotian Xu,
            Oluwatosin Oseni,
            Yang Gao,
            Tao Zhang
              <br>
              <span style="font-size: 16px;color: red;"><em>CoRL</em></span>, 2024</span>
              <br>
              <a href="https://dexcatch.github.io/">Project Page</a>
              /
              <a href="https://arxiv.org/abs/2310.08809">ArXiv</a>
              /
              <a href="https://dexcatch.github.io/">Code</a>
              /
              <!-- <a href="https://youtu.be/2mmqQYO4KlY">Video</a> -->
              <p></p>
              <p>
                We present a learning-based catching strategy, which can catch diverse objects of daily life with dexterous hands. The learned policies show strong zero-shot transfer performance on unseen objects.
              </p>
          </td>
        </tr>
	      
        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/copa.png' width="100%">
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2403.08248">
              <papertitle>CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models</papertitle>
            </a>
            <br>
            Haoxu Huang*, 
            Fanqi Lin*, 
            Yingdong Hu,
            <strong>Shengjie Wang</strong>,
            Yang Gao
              <br>
              <span style="font-size: 16px;color: red;"><em>IROS Oral</em></span>, 2024</span>
              <br>
              <a href="https://copa-2024.github.io/">Project website</a>
              / 
              <a href="https://twitter.com/gao_young/status/1768411722036253181">Twitter</a>
              /
              <a href="https://arxiv.org/abs/2403.08248">ArXiv</a>
              /
              <a href="https://copa-2024.github.io/">Code</a>
              /
              <!-- <a href="https://youtu.be/2mmqQYO4KlY">Video</a> -->
              <p></p>
              <p>
                We propose Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that incorporates common sense knowledge embedded within foundation vision-language models (VLMs), such as GPT-4V, into the low-level robotic manipulation tasks.
              </p>
          </td>
        </tr>


        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/EZ.png' width="100%">
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2403.00564">
              <papertitle>EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data</papertitle>
            </a>
            <br>
            <strong>Shengjie Wang*</strong>,
            Shaohuai Liu*, 
            Weirui Ye*,
            Jiacheng You,
            Yang Gao
              <br>
              <span style="font-size: 16px;color: red;"><em>ICML Spotlight (Top 3.5%)</em></span>, 2024</span>
              <br>
              <a href="https://twitter.com/arankomatsuzaki/status/1764508918711992421">Twitter</a>
              /
              <a href="https://arxiv.org/abs/2403.00564">ArXiv</a>
              /
              <a href="https://github.com/Shengjiewang-Jason/EfficientZeroV2">Code</a>
              /
              <!-- <a href="https://youtu.be/2mmqQYO4KlY">Video</a> -->
              <p></p>
              <p>
                Introducing EfficientZero V2, a general framework designed for sample-efficient RL algorithms, which outperforms the SotA methods and DreamerV3 across diverse domains.
              </p>
          </td>
        </tr>


        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/ioctree.gif' width="100%">
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://sites.google.com/view/I-Octree">
              <papertitle>I-Octree: A Fast, Lightweight,  and Dynamic  Octree for Proximity  Search</papertitle>
            </a>
            <br>
            Jun Zhu, 
            Hongyi Li, 
            Zhepeng Wang, 
            <strong>Shengjie Wang </strong>,
            Tao Zhang
              <br>
              <span style="font-size: 16px;color: red;"><em>ICRA</em></span>, 2024</span>
              <br>
              <a href="https://sites.google.com/view/I-Octree">Project website</a>
              /
              <a href="https://arxiv.org/abs/2309.08315">ArXiv</a>
              /
              <a href="https://github.com/zhujun3753/i-octree">Code</a>
              /
              <!-- <a href="https://youtu.be/2mmqQYO4KlY">Video</a> -->
              <p></p>
              <p>
                we present the i-Octree, a dynamic octree data structure that supports both fast nearest neighbor search and real-time dynamic updates, outperforming contemporary state-of-the-art approaches by achieving, on average, a <strong>19%</strong> reduction in runtime on realworld open datasets.
              </p>
          </td>
        </tr>        

        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/stable.png' width="100%">
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://sites.google.com/view/adaptive-lyapunov-actor-critic">
              <papertitle>A Policy Optimization Method Towards Optimal-time Stability</papertitle>
            </a>
            <br> 
            <strong>Shengjie Wang</strong>,
            Fengbo Lan,
            Xiang Zheng,
            Yuxue Cao,
            Oluwatosin Oseni,
            Haotian Xu,
            Tao Zhang,
            Yang Gao
            <br>
            <span style="font-size: 16px;color: red;"><em>CoRL</em></span>, 2023</span>
            <br>
            <a href="https://sites.google.com/view/adaptive-lyapunov-actor-critic">Project Page</a>
            /
            <a href="https://arxiv.org/abs/2301.00521">ArXiv</a>
            /
            <a href="https://github.com/Tsinghua-Space-Robot-Learning-Group/Adaptive-Lyapunov-based-Actor-Critic">Code</a>
            <p></p>
            <p>
              Our approach enables the system's state to reach an equilibrium point within an optimal time and maintain stability there- after, referred to as <em>"optimal-time stability"</em>. To achieve this, we integrate the optimization method into the Actor-Critic framework, resulting in the development of the Adaptive Lyapunov-based Actor-Critic (ALAC) algorithm.
            </p>
          </td>
        </tr>

        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/LCPO.png' width="100%">
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://sites.google.com/view/esb-cpo">
              <papertitle>Efficient Exploration Using Extra Safety Budget in Constrained Policy Optimization</papertitle>
            </a>
            <br> 
            Haotian Xu*,
            <strong>Shengjie Wang*</strong>,
            Zhaolei Wang,
            Yunzhe Zhang,
            Qing Zhuo,
            Yang Gao,
            Tao Zhang
            <br>
            <span style="font-size: 16px; color: red;"><em>IROS</em></span>, 2023</span>
            <br>
            <a href="https://sites.google.com/view/esb-cpo">Project Page</a>
            /
            <a href="https://arxiv.org/abs/2302.14339">ArXiv</a>
            /
            <a href="https://github.com/Tsinghua-Space-Robot-Learning-Group/ESB-CPO">Code</a>
            <p></p>
            <p>
              Our algorithm (ESB-CPO) improves upon the trade-off between reducing constraint violations and improving expected returns in Safe Reinforcement Learning.
            </p>
          </td>
        </tr>        

        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/LAC.gif' width="100%">
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://sites.google.com/view/lac-manipulation">
              <papertitle>A Learning-based Adaptive Compliance Method for Symmetric Bi-manual Manipulation</papertitle>
            </a>
            <br> 
            Yuxue Cao*,
            <strong>Shengjie Wang*</strong>,
            Xiang Zheng,
            Wenke Ma,
            Tao Zhang
            <br>
            <span style="font-size: 16px; color: red;"><em>T-ASE (Top Journal in Automation)</em></span>, Under review</span>
            <br>
            <a href="https://sites.google.com/view/lac-manipulation">Project Page</a>
            /
            <a href="https://arxiv.org/abs/2303.15262">ArXiv</a>
            /
            <a href="https://github.com/Tsinghua-Space-Robot-Learning-Group/SpaceRobotEnv">Code</a>
            <p></p>
            <p>
              We propose a novel Learning-based Adaptive Compliance (LAC) algorithm to improve the efficiency and adaptability of symmetric bi-manual manipulation.
            </p>
          </td>
        </tr>
        
        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/IMAP.png' width="100%">
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2305.02605.pdf">
              <papertitle>IMAP: Intrinsically Motivated Adversarial Policy</papertitle>
            </a>
            <br> 
            Xiang Zheng, 
            Xingjun Ma, 
            <strong>Shengjie Wang</strong>, 
            Xinyu Wang, 
            Chao Shen, 
            Cong Wang
            <br>
            <span style="font-size: 16px; color: red;"><em>ACM CCS (Top Conference in Security)</em></span>, Under review</span>
            <br>
            <a href="https://arxiv.org/pdf/2305.02605">Project Page</a>
            /
            <a href="https://arxiv.org/pdf/2305.02605.pdf">ArXiv</a>
            /
            <a href="https://arxiv.org/pdf/2305.02605">Code</a>
            <p></p>
            <p>
              We propose the Intrinsically Motivated Adversarial Policy (IMAP) for efficient black-box evasion attacks in single- and multi-agent environments without any knowledge of the victim policy.
            </p>
          </td>
        </tr>          
          
          <tr>
            <td style="padding:10px;width:30%;vertical-align:middle">
              <img src='images/roboticrat.gif' width="100%">
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/esb-cpo">
                <papertitle>Development of a small-sized quadruped robotic rat capable of multimodal motions</papertitle>
              </a>
              <br>
              <strong>Shengjie Wang</strong>,
              Qing Shi,
              Junhui Gao, 
              Yuxuan Wang, 
              Fansheng Meng, 
              Chang Li, 
              Qiang Huang, 
              Toshio Fukuda
              <br>
              <span style="font-size: 16px;"><em>Journal: <span style="color: red;">T-RO (Top journal in Robotics)</span></em>, 2023</span> <br>
              <span style="font-size: 16px;"><em>Conference: Advanced Intelligent Mechatronics (AIM), <font color="DeepPink">(Best Student Paper Award(Top 0.5%))</font> </em>, 2019</span>
              <br>
              <a href="https://spectrum.ieee.org/robotic-rat-climbs-crawls-turns">IEEE Spectrum</a>
              /
              <a href="https://www.eurekalert.org/news-releases/950018">EurekAlert</a>
              /
              <a href="https://www.sciencetimes.com/articles/37311/20220425/small-quadruped-robot-squro-developed-mimic-agility-rats-carry-objects.htm">Science Times</a>
              <p></p>
              <p>
                We developed a small-sized quadruped robotic rat (SQuRo), which includes four limbs and one flexible spine. On the basis of the extracted key movement joints, SQuRo was subtly designed with a relatively elongated slim body (aspect ratio: 3.42) and smaller weight (220 g) compared with quadruped robots of the same scale.
              </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:10px;width:30%;vertical-align:middle">
              <img src='images/EfficientLPT.gif' width="100%">
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2209.01434">
                <papertitle>Reinforcement learning with prior policy guidance for motion planning of dual-arm free-floating space robot</papertitle>
              </a>
              <br>
              Yuxue Cao*,
              <strong>Shengjie Wang*</strong>,
              Xiang Zheng, 
              Wenke Ma, 
              Xinru Xie, 
              Lei Liu
              <br>
              <span style="font-size: 16px;color: red;"><em> AST (Top journal in Astronautics)</span></em>, 2023</span>
              <br>
              <a href="https://arxiv.org/abs/2209.01434">ArXiv</a>
              /
              <a href="https://github.com/Tsinghua-Space-Robot-Learning-Group/SpaceRobotEnv">Code</a>
              <p></p>
              <p>
                We propose a novel algorithm, EfficientLPT, to facilitate RL-based methods to improve planning accuracy efficiently. Our core contributions are constructing a mixed policy with prior knowledge guidance and introducing infinite norm to build a more reasonable reward function.
              </p>
            </td>
          </tr>          

          <tr>
            <td style="padding:10px;width:30%;vertical-align:middle">
              <img src='images/bidexhands.gif' width="100%">
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://bi-dexhands.ai/">
                <papertitle>Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning</papertitle>
              </a>
              <br>
              Yuanpei Chen,
              Tianhao Wu, 
							<strong>Shengjie Wang</strong>,
              Xidong Feng,
              Jiechuang Jiang,
              Stephen Marcus McAleer,
              Hao Dong,
              Zongqing Lu,
              Song-chun Zhu,
              Yaodong Yang
              <br>
              <span style="font-size: 16px;color: red;"><em>NeurIPS</em></span>, 2022</span>
              <br>
              <a href="https://bi-dexhands.ai/">Project Page</a>
              /
              <a href="https://arxiv.org/abs/2206.08686">ArXiv</a>
              /
              <a href="https://github.com/PKU-MARL/DexterousHands">Code</a>
              <p></p>
              <p>
                We propose a bimanual dexterous manipulation benchmark (Bi-DexHands) according to literature from cognitive science for comprehensive reinforcement learning research.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:30%;vertical-align:middle">
              <img src='images/ral.gif' width="100%">
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9718193">
                <papertitle>Collision-Free Trajectory Planning for a 6-DoF Free Floating Space Robot via Hierarchical Decoupling Optimization</papertitle>
              </a>
              <br>
              <strong>Shengjie Wang</strong>,
              Yuxue Cao, 
              Xiang Zheng, 
              Tao Zhang
              <br>
              <span style="font-size: 16px;color: red;"><em>IEEE RA-L</span></em>, 2022</span>
              <br>
              <a href="https://qq1242063902.wixsite.com/website">Project Page</a>
              /
              <a href="https://ieeexplore.ieee.org/abstract/document/9718193">Paper</a>
              /
              <a href="https://github.com/Tsinghua-Space-Robot-Learning-Group/SpaceRobotEnv">Code</a>
              <p></p>
              <p>
                We developed a model-free Hierarchical Decoupling Optimization (HDO) algorithm to realize 6D-pose multi-target trajectory planning for the free-floating space robot. 
              </p>
            </td>
          </tr>

        </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>
              <span class="php">Experience</span></heading>
          </td>
        </tr>
      </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="10"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/Tsinghua.png", width="90%"></td>
            <td width="80%" valign="center">
              <b>Tsinghua University</b>, China
              <br> 2019.09 - Present
              <br>
              <br> <b>Master Student and PhD Student</b>
              <br> Advisor: Prof. <a href="http://people.iiis.tsinghua.edu.cn/~gaoyang/yang-gao.weebly.com/index.html">Yang Gao</a> and Prof. <a href="https://www.au.tsinghua.edu.cn/info/1110/1573.htm">Tao Zhang</a>(IET Fellow, Head of Department)
            </td>
          </tr>
          <tr>
            <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/BIT.jpg", width="90%"></td>
            <td width="80%" valign="center">
              <b>Beijing Institute of Technology</b>, China
              <br> 2015.09 - 2019.07
              <br>
              <br> <b>Undergraduate Student </b>
              <br> Advisor: Prof. <a href="https://smen.bit.edu.cn/sztd/szms/znjqryjs/b101011.htm">Qing Shi</a> (IEEE Senior Member) and Prof. <a href="http://toshiofukuda.org">Toshio Fukuda</a> (2020 IEEE President).
            </td>
          </tr>

        </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
              Template stolen from <a href="https://jonbarron.info/">Jon Barron</a>.
              <br> Last updated: Oct 15, 2023 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
